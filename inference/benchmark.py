#!/usr/bin/env python3
"""
OpenPangu Medusa æ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬ (å·²é€‚é…æ˜‡è…¾NPUéªŒæ”¶)

å¯¹æ¯”é›†æˆ Medusa Heads å‰åçš„æ¨ç†æ€§èƒ½ï¼š
- generate.py: åŸå§‹ OpenPangu æ¨¡å‹ï¼ˆè‡ªå›å½’è§£ç ï¼‰
- medusa_generate.py: é›†æˆ Medusa çš„æ¨¡å‹ï¼ˆæŠ•æœºè§£ç ï¼‰

æµ‹è¯•æŒ‡æ ‡ï¼š
- TPOT (Time Per Output Token): æ¯ä¸ª token çš„ç”Ÿæˆæ—¶é—´ (ms)
- TPS (Tokens Per Second): æ¯ç§’ç”Ÿæˆçš„ token æ•°
- TTFT (Time To First Token): é¦–ä¸ª token å»¶è¿Ÿ (ms)
- æ€»ç”Ÿæˆæ—¶é—´
- åŠ é€Ÿæ¯” (Speedup)
"""

import argparse
import time
import torch
import sys
import os
from pathlib import Path

# --- æ˜‡è…¾ NPU é€‚é…ä¸éªŒæ”¶ä¿¡æ¯å‡†å¤‡ ---
try:
    import torch_npu
    from torch_npu.contrib import transfer_to_npu
    DEVICE_TYPE = "npu"
    DEVICE_TAG = "æ˜‡è…¾ (Ascend NPU)"
    print(f"[Info] æ£€æµ‹åˆ° torch_npuï¼Œå°†ä½¿ç”¨æ˜‡è…¾ NPU è¿›è¡Œæ¨ç†ã€‚")
except ImportError:
    DEVICE_TYPE = "cuda" if torch.cuda.is_available() else "cpu"
    DEVICE_TAG = "NVIDIA CUDA" if DEVICE_TYPE == "cuda" else "CPU"
    print(f"[Warning] æœªæ£€æµ‹åˆ° torch_npuï¼Œé€€åŒ–ä½¿ç”¨ {DEVICE_TAG}ã€‚")

target_device = f"{DEVICE_TYPE}:0"

def device_synchronize():
    """è·¨å¹³å°è®¾å¤‡åŒæ­¥å‡½æ•°"""
    if DEVICE_TYPE == 'npu':
        torch.npu.synchronize()
    elif DEVICE_TYPE == 'cuda':
        torch.cuda.synchronize()

def empty_cache():
    """è·¨å¹³å°æ¸…ç©ºæ˜¾å­˜å‡½æ•°"""
    if DEVICE_TYPE == 'npu':
        torch.npu.empty_cache()
    elif DEVICE_TYPE == 'cuda':
        torch.cuda.empty_cache()

def print_acceptance_info(model_path, is_parallel=False):
    """æ‰“å°ç¬¬ä¸€é˜¶æ®µéªŒæ”¶æ‰€éœ€çš„å…³é”®ä¿¡æ¯ç”¨äºæˆªå›¾"""
    mode_str = "æ”¯æŒå¹¶è¡Œæ¨ç† (Medusa)" if is_parallel else "åŸºå‡†ä¸²è¡Œæ¨ç† (Baseline)"
    print("\n" + "=" * 60)
    print(f"1. è¿è¡Œæ¨¡å¼: {mode_str}")
    print(f"2. è¿è¡Œç¯å¢ƒ: {DEVICE_TAG}, Device: {target_device})")
    print(f"3. æ¨¡å‹çŠ¶æ€: åŠ è½½æ¨¡å‹ä¸ºopenPanguç³»åˆ—å¼€æºæ¨¡å‹")
    print(f"   - æ¨¡å‹è·¯å¾„: {model_path}")
    print("=" * 60 + "\n")
# ------------------------------------

# æ·»åŠ çˆ¶ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, str(Path(__file__).parent.parent))

def benchmark_baseline(model_path, prompt, max_new_tokens, num_runs=3, warmup_runs=1):
    """æµ‹è¯•åŸå§‹æ¨¡å‹ï¼ˆæ—  Medusaï¼‰"""
    from transformers import AutoModelForCausalLM, AutoTokenizer, AutoConfig
    
    print("=" * 60)
    print(f"Baseline: OpenPangu (Autoregressive Decoding) on {DEVICE_TAG}")
    print("=" * 60)
    
    # åŠ è½½æ¨¡å‹
    print("Loading model...")
    tokenizer = AutoTokenizer.from_pretrained(
        model_path, 
        trust_remote_code=True,
        use_fast=False,
    )
    model = AutoModelForCausalLM.from_pretrained(
        model_path,
        torch_dtype=torch.float16,
        device_map=target_device, # é€‚é… NPU
        trust_remote_code=True,
    )
    model.eval()

    # ã€éªŒæ”¶æˆªå›¾å…³é”®ç‚¹ã€‘æ‰“å°éªŒæ”¶ä¿¡æ¯
    print_acceptance_info(model_path, is_parallel=False)
    
    # å‡†å¤‡è¾“å…¥
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚"},
        {"role": "user", "content": prompt},
    ]
    formatted_prompt = tokenizer.apply_chat_template(
        messages, tokenize=False, add_generation_prompt=True
    )
    # é€‚é… NPU
    input_ids = tokenizer.encode(formatted_prompt, return_tensors="pt").to(target_device)
    input_len = input_ids.shape[1]
    
    print(f"Input length: {input_len} tokens")
    print(f"Max new tokens: {max_new_tokens}")
    
    # OpenPangu ç‰¹å®šçš„ eos_token_id
    eos_token_id = 45892
    
    # Warmup
    print(f"\nWarmup ({warmup_runs} runs)...")
    for _ in range(warmup_runs):
        with torch.no_grad():
            _ = model.generate(
                input_ids,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                eos_token_id=eos_token_id,
            )
    
    # Benchmark
    print(f"Benchmarking ({num_runs} runs)...")
    times = []
    output_tokens_list = []
    ttft_list = []
    
    for i in range(num_runs):
        device_synchronize() # é€‚é… NPU åŒæ­¥
        
        # æµ‹é‡ TTFTï¼ˆé¦–ä¸ª token æ—¶é—´ï¼‰
        start_time = time.perf_counter()
        
        with torch.no_grad():
            outputs = model.generate(
                input_ids,
                max_new_tokens=max_new_tokens,
                do_sample=False,
                eos_token_id=eos_token_id,
            )
        
        device_synchronize() # é€‚é… NPU åŒæ­¥
        end_time = time.perf_counter()
        
        elapsed = end_time - start_time
        output_tokens = outputs.shape[1] - input_len
        
        times.append(elapsed)
        output_tokens_list.append(output_tokens)
        
        print(f"  Run {i+1}: {elapsed:.3f}s, {output_tokens} tokens")
    
    # è®¡ç®—ç»Ÿè®¡
    avg_time = sum(times) / len(times)
    avg_tokens = sum(output_tokens_list) / len(output_tokens_list)
    tps = avg_tokens / avg_time
    tpot = (avg_time / avg_tokens) * 1000  # ms per token
    
    results = {
        "method": "Baseline (Autoregressive)",
        "avg_time": avg_time,
        "avg_tokens": avg_tokens,
        "tps": tps,
        "tpot": tpot,
    }
    
    print(f"\n--- Baseline Results ---")
    print(f"Average time: {avg_time:.3f}s")
    print(f"Average tokens: {avg_tokens:.1f}")
    print(f"TPS: {tps:.2f} tokens/s")
    print(f"TPOT: {tpot:.2f} ms/token")
    
    # æ¸…ç†æ˜¾å­˜
    del model
    empty_cache() # é€‚é… NPU æ¸…ç†æ˜¾å­˜
    
    return results


def benchmark_medusa(model_path, medusa_dir, prompt, max_new_tokens, num_runs=3, warmup_runs=1):
    """æµ‹è¯• Medusa æ¨¡å‹ï¼ˆæŠ•æœºè§£ç ï¼‰"""
    from medusa_generate import MedusaPanguInference
    from medusa_choices import pangu_stage2
    
    print("\n" + "=" * 60)
    print(f"Medusa: OpenPangu + Medusa Heads (Speculative Decoding) on {DEVICE_TAG}")
    print("=" * 60)
    
    # åŠ è½½æ¨¡å‹
    print("Loading model...")
    model = MedusaPanguInference(
        base_model_path=model_path,
        medusa_head_path=os.path.join(medusa_dir, "medusa_lm_head.safetensors"),
        tokenizer_path=medusa_dir,
        device=target_device, # é€‚é… NPU
        dtype=torch.float16,
        medusa_num_heads=3,
        medusa_num_layers=1,
    )

    # ã€éªŒæ”¶æˆªå›¾å…³é”®ç‚¹ã€‘æ‰“å°éªŒæ”¶ä¿¡æ¯
    print_acceptance_info(model_path, is_parallel=True)
    
    # å‡†å¤‡è¾“å…¥
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚"},
        {"role": "user", "content": prompt},
    ]
    formatted_prompt = model.apply_chat_template(messages)
    # é€‚é… NPU
    input_ids = model.tokenizer.encode(formatted_prompt, return_tensors="pt").to(target_device)
    input_len = input_ids.shape[1]
    
    print(f"Input length: {input_len} tokens")
    print(f"Max steps: {max_new_tokens}")
    
    # Warmup
    print(f"\nWarmup ({warmup_runs} runs)...")
    for _ in range(warmup_runs):
        _ = model.generate(formatted_prompt, max_steps=max_new_tokens, temperature=0.0)
    
    # Benchmark
    print(f"Benchmarking ({num_runs} runs)...")
    times = []
    output_tokens_list = []
    accepted_tokens_list = []
    
    for i in range(num_runs):
        device_synchronize() # é€‚é… NPU åŒæ­¥
        start_time = time.perf_counter()
        
        output_text = model.generate(
            formatted_prompt, 
            max_steps=max_new_tokens, 
            temperature=0.0,
            medusa_choices=pangu_stage2,
        )
        
        device_synchronize() # é€‚é… NPU åŒæ­¥
        end_time = time.perf_counter()
        
        elapsed = end_time - start_time
        
        # è®¡ç®—è¾“å‡º token æ•°
        output_ids = model.tokenizer.encode(output_text, return_tensors="pt")
        output_tokens = output_ids.shape[1] - input_len
        
        times.append(elapsed)
        output_tokens_list.append(output_tokens)
        
        print(f"  Run {i+1}: {elapsed:.3f}s, {output_tokens} tokens")
    
    # è®¡ç®—ç»Ÿè®¡
    avg_time = sum(times) / len(times)
    avg_tokens = sum(output_tokens_list) / len(output_tokens_list)
    tps = avg_tokens / avg_time
    tpot = (avg_time / avg_tokens) * 1000  # ms per token
    
    results = {
        "method": "Medusa (Speculative Decoding)",
        "avg_time": avg_time,
        "avg_tokens": avg_tokens,
        "tps": tps,
        "tpot": tpot,
    }
    
    print(f"\n--- Medusa Results ---")
    print(f"Average time: {avg_time:.3f}s")
    print(f"Average tokens: {avg_tokens:.1f}")
    print(f"TPS: {tps:.2f} tokens/s")
    print(f"TPOT: {tpot:.2f} ms/token")
    
    # æ¸…ç†æ˜¾å­˜
    del model
    empty_cache() # é€‚é… NPU æ¸…ç†æ˜¾å­˜
    
    return results


def main():
    parser = argparse.ArgumentParser(description="Benchmark OpenPangu with/without Medusa")
    parser.add_argument("--base_model", type=str, default="/root/openPangu-Embedded-7B-V1.1",
                        help="Base model path")
    parser.add_argument("--medusa_dir", type=str, 
                        default="/root/OpenPangu7B-on-NVIDIA/test_medusa_mlp_._medusa_3_lr_0.001_layers_1",
                        help="Medusa head directory")
    parser.add_argument("--prompt", type=str, 
                        default="è¯·è¯¦ç»†ä»‹ç»ä¸€ä¸‹å¤§è¯­è¨€æ¨¡å‹çš„å·¥ä½œåŸç†ã€‚",
                        help="Test prompt")
    parser.add_argument("--max_tokens", type=int, default=256,
                        help="Maximum new tokens to generate")
    parser.add_argument("--num_runs", type=int, default=3,
                        help="Number of benchmark runs")
    parser.add_argument("--warmup", type=int, default=1,
                        help="Number of warmup runs")
    parser.add_argument("--baseline_only", action="store_true",
                        help="Only run baseline benchmark")
    parser.add_argument("--medusa_only", action="store_true",
                        help="Only run Medusa benchmark")
    args = parser.parse_args()
    
    # è§£æè·¯å¾„
    base_model_path = str(Path(args.base_model).expanduser().resolve())
    medusa_dir = str(Path(args.medusa_dir).expanduser().resolve())
    
    print("=" * 60)
    print(f"OpenPangu + Medusa Performance Benchmark on {DEVICE_TAG}")
    print("=" * 60)
    print(f"Base model: {base_model_path}")
    print(f"Medusa dir: {medusa_dir}")
    print(f"Prompt: {args.prompt[:50]}...")
    print(f"Max tokens: {args.max_tokens}")
    print(f"Runs: {args.num_runs} (+ {args.warmup} warmup)")
    
    results = {}
    
    # Baseline benchmark
    if not args.medusa_only:
        results["baseline"] = benchmark_baseline(
            base_model_path, 
            args.prompt, 
            args.max_tokens,
            args.num_runs,
            args.warmup,
        )
    
    # Medusa benchmark
    if not args.baseline_only:
        results["medusa"] = benchmark_medusa(
            base_model_path,
            medusa_dir,
            args.prompt,
            args.max_tokens,
            args.num_runs,
            args.warmup,
        )
    
    # å¯¹æ¯”ç»“æœ
    if "baseline" in results and "medusa" in results:
        print("\n" + "=" * 60)
        print("COMPARISON SUMMARY")
        print("=" * 60)
        
        baseline = results["baseline"]
        medusa = results["medusa"]
        
        speedup_tps = medusa["tps"] / baseline["tps"]
        speedup_time = baseline["avg_time"] / medusa["avg_time"]
        
        print(f"\n{'Metric':<25} {'Baseline':<15} {'Medusa':<15} {'Speedup':<10}")
        print("-" * 65)
        print(f"{'TPS (tokens/s)':<25} {baseline['tps']:<15.2f} {medusa['tps']:<15.2f} {speedup_tps:<10.2f}x")
        print(f"{'TPOT (ms/token)':<25} {baseline['tpot']:<15.2f} {medusa['tpot']:<15.2f} {baseline['tpot']/medusa['tpot']:<10.2f}x")
        print(f"{'Total time (s)':<25} {baseline['avg_time']:<15.3f} {medusa['avg_time']:<15.3f} {speedup_time:<10.2f}x")
        print(f"{'Tokens generated':<25} {baseline['avg_tokens']:<15.1f} {medusa['avg_tokens']:<15.1f}")
        
        print(f"\nğŸš€ Medusa achieves {speedup_tps:.2f}x speedup!")
        
        if speedup_tps > 1.5:
            print("âœ… Significant speedup achieved!")
        elif speedup_tps > 1.0:
            print("âš ï¸  Moderate speedup. Consider tuning Medusa parameters.")
        else:
            print("âŒ No speedup. Check Medusa head training quality.")


if __name__ == "__main__":
    main()
    