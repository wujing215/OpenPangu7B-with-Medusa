#!/usr/bin/env python3
"""
å¿«é€Ÿè‡ªè’¸é¦æ•°æ®ç”Ÿæˆè„šæœ¬ - ä¼˜åŒ–ç‰ˆ

ä¼˜åŒ–ç‚¹ï¼š
1. å‡å°‘ max_new_tokensï¼ˆ512 è€Œä¸æ˜¯ 1024ï¼‰
2. æ”¯æŒå¤šè¿›ç¨‹å¹¶è¡Œï¼ˆå¤š GPUï¼‰
3. æ·»åŠ è¶…æ—¶æœºåˆ¶
4. æ›´é¢‘ç¹çš„ä¿å­˜
"""

import json
import torch
import argparse
from tqdm import tqdm
from transformers import AutoModelForCausalLM, AutoTokenizer
from pathlib import Path
import time

# ================= é…ç½® =================
MODEL_PATH = str(Path(__file__).parent.parent.resolve())
INPUT_DATA = "third_party/ShareGPT_Vicuna_unfiltered/ShareGPT_V4.3_unfiltered_cleaned_split.json"
OUTPUT_DATA = "pangu_self_distilled_data.json"
DEVICE = "cuda"
NUM_SAMPLES = 50000
MAX_NEW_TOKENS = 512  # ğŸ”¥ å‡å°‘åˆ° 512ï¼ˆä» 1024ï¼‰
BATCH_SIZE = 1
TIMEOUT_SECONDS = 60  # å•ä¸ªæ ·æœ¬è¶…æ—¶æ—¶é—´

# å…¨å±€å˜é‡
tokenizer = None
model = None


def generate_response(prompt_text, timeout=TIMEOUT_SECONDS):
    """ä½¿ç”¨ OpenPangu åŸæ¨¡å‹ç”Ÿæˆå›å¤ï¼ˆè‡ªè’¸é¦ï¼‰+ è¶…æ—¶ä¿æŠ¤"""
    messages = [
        {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªæœ‰å¸®åŠ©çš„åŠ©æ‰‹ã€‚"},
        {"role": "user", "content": prompt_text}
    ]
    
    formatted_text = tokenizer.apply_chat_template(
        messages,
        tokenize=False,
        add_generation_prompt=True
    )
    
    inputs = tokenizer(formatted_text, return_tensors="pt").to(DEVICE)
    
    start_time = time.time()
    
    try:
        with torch.no_grad():
            outputs = model.generate(
                **inputs, 
                max_new_tokens=MAX_NEW_TOKENS,  # å‡å°‘åˆ° 512
                do_sample=False,
                eos_token_id=45892,
                return_dict_in_generate=True
            )
        
        # æ£€æŸ¥æ˜¯å¦è¶…æ—¶
        elapsed = time.time() - start_time
        if elapsed > timeout:
            print(f"\nâš ï¸  Generation timeout ({elapsed:.1f}s), skipping...")
            return None
        
        input_length = inputs.input_ids.shape[1]
        generated_ids = outputs.sequences[0, input_length:]
        output_sent = tokenizer.decode(generated_ids, skip_special_tokens=False)
        
        # è§£æ OpenPangu æ ¼å¼
        try:
            thinking_content = output_sent.split("[unused17]")[0].split("[unused16]")[-1].strip()
            content = output_sent.split("[unused17]")[-1].split("[unused10]")[0].strip()
            return content if content else output_sent
        except:
            return tokenizer.decode(generated_ids, skip_special_tokens=True)
    
    except Exception as e:
        print(f"\nâŒ Generation error: {e}")
        return None


def main():
    global MODEL_PATH, INPUT_DATA, OUTPUT_DATA, NUM_SAMPLES, MAX_NEW_TOKENS, DEVICE, tokenizer, model
    
    parser = argparse.ArgumentParser(description="Generate self-distillation data (Fast version)")
    parser.add_argument("--model_path", type=str, default=MODEL_PATH)
    parser.add_argument("--input_data", type=str, default=INPUT_DATA)
    parser.add_argument("--output_data", type=str, default=OUTPUT_DATA)
    parser.add_argument("--num_samples", type=int, default=NUM_SAMPLES)
    parser.add_argument("--max_new_tokens", type=int, default=MAX_NEW_TOKENS)
    parser.add_argument("--device", type=str, default=DEVICE)
    parser.add_argument("--start_idx", type=int, default=0, help="Start index for parallel processing")
    parser.add_argument("--end_idx", type=int, default=None, help="End index for parallel processing")
    args = parser.parse_args()
    
    MODEL_PATH = args.model_path
    INPUT_DATA = args.input_data
    OUTPUT_DATA = args.output_data
    NUM_SAMPLES = args.num_samples
    MAX_NEW_TOKENS = args.max_new_tokens
    DEVICE = args.device
    
    print("=" * 60)
    print("Fast Self-Distillation Data Generator")
    print("=" * 60)
    print(f"Model: {MODEL_PATH}")
    print(f"Max tokens: {MAX_NEW_TOKENS}")
    print(f"Device: {DEVICE}")
    
    # åŠ è½½æ¨¡å‹
    print("\nLoading model...")
    tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH, trust_remote_code=True, use_fast=False)
    model = AutoModelForCausalLM.from_pretrained(
        MODEL_PATH, 
        device_map="auto", 
        torch_dtype=torch.float16, 
        trust_remote_code=True
    )
    model.eval()
    print("âœ… Model loaded!")
    
    # åŠ è½½æ•°æ®
    with open(INPUT_DATA, 'r', encoding='utf-8') as f:
        original_data = json.load(f)
    
    # ç¡®å®šå¤„ç†èŒƒå›´
    start_idx = args.start_idx
    end_idx = args.end_idx if args.end_idx else min(NUM_SAMPLES, len(original_data))
    data_to_process = original_data[start_idx:end_idx]
    
    print(f"\nProcessing samples {start_idx} to {end_idx} ({len(data_to_process)} total)")
    
    new_data = []
    skipped = 0
    
    for i, conversation in enumerate(tqdm(data_to_process, desc="Generating")):
        actual_idx = start_idx + i
        
        try:
            user_msg = next((msg for msg in conversation["conversations"] if msg["from"] == "human"), None)
            
            if user_msg:
                prompt = user_msg["value"]
                
                # ç”Ÿæˆå›å¤
                pangu_response = generate_response(prompt)
                
                if pangu_response is None:
                    skipped += 1
                    continue
                
                new_conv_entry = {
                    "id": conversation.get("id", f"gen_{actual_idx}"),
                    "conversations": [
                        {"from": "human", "value": prompt},
                        {"from": "gpt", "value": pangu_response}
                    ]
                }
                
                new_data.append(new_conv_entry)
                
                # æ¯ 50 ä¸ªæ ·æœ¬ä¿å­˜ä¸€æ¬¡ï¼ˆæ›´é¢‘ç¹ï¼‰
                if len(new_data) % 50 == 0 and len(new_data) > 0:
                    with open(OUTPUT_DATA, 'w', encoding='utf-8') as out_f:
                        json.dump(new_data, out_f, indent=2, ensure_ascii=False)
                    print(f"\nğŸ’¾ Checkpoint: Saved {len(new_data)} samples (skipped: {skipped})")
                        
        except Exception as e:
            print(f"\nâŒ Error at {actual_idx}: {e}")
            skipped += 1
            continue
    
    # æœ€ç»ˆä¿å­˜
    with open(OUTPUT_DATA, 'w', encoding='utf-8') as out_f:
        json.dump(new_data, out_f, indent=2, ensure_ascii=False)
    
    print("\n" + "=" * 60)
    print("âœ… Generation Complete!")
    print(f"   Generated: {len(new_data)} samples")
    print(f"   Skipped: {skipped} samples")
    print(f"   Output: {OUTPUT_DATA}")
    print("=" * 60)


if __name__ == "__main__":
    main()
